# -*- coding: utf-8 -*-
"""TTIC31210_HW1_1-3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tEzb3GYTWATCCljPk2pjssot8la6UKiM

# TTIC 31210 HW1 Language Modeling Part 1/2

Name: Yi Hung Liu

- UCID: yhliu
- Email: yhliu@uchicago.edu


---


For reading convenience, I suggest read this on online [Colab notebook](https://colab.research.google.com/drive/1tEzb3GYTWATCCljPk2pjssot8la6UKiM) and fold all the "Code" and "Training printout" section (e.g. "1.0 Code") so that only essential descriptions, parameters, graphs and trained results are displayed (where the title number is matched with the assignment description). Or use the table of contents sidebar (View -> Table of contents).

I have separated this assignment to two notebooks for training at the same time. The **first part is here containing section 1-3**. The **second part is [there](https://colab.research.google.com/drive/1dpFW7cimQi72p_XGpOVZMDBbOQCOYXJp) containing section 4-5**. Sorry for the convenience.

# 1 Log Loss

## 1.0 Code

### 1.0.1 Loading data and preprocessing
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import csv
import string
import time
import matplotlib.pyplot as plt

FROM_GOOGLE_DRIVE = True
if FROM_GOOGLE_DRIVE:
    from google.colab import drive
    drive.mount('/content/drive')
    drive_dir = '/content/drive/My Drive/'
else:
    drive_dir = '/Users/T/Google Drive/'
data_dir = drive_dir + '02 UChicago/3 19Spring/TTIC 31210 Advanced NLP/HW1/'

def read_data():
    '''load and split data'''
    lm_dev = []
    lm_test = []
    lm_train = []
    
    lm_dev = np.loadtxt(data_dir + "31210-s19-hw1/bobsue.lm.dev.txt", dtype = np.dtype(str), delimiter = "\n")
    lm_dev = [x.split() for x in lm_dev]
    lm_test = np.loadtxt(data_dir + "31210-s19-hw1/bobsue.lm.test.txt", dtype = np.dtype(str), delimiter = "\n")
    lm_test = [x.split() for x in lm_test]
    lm_train = np.loadtxt(data_dir + "31210-s19-hw1/bobsue.lm.train.txt", dtype = np.dtype(str), delimiter = "\n")
    lm_train = [x.split() for x in lm_train]

    prevsent_dev = []
    prevsent_test = []
    prevsent_train = []

    prevsent_dev = np.loadtxt(data_dir + "31210-s19-hw1/bobsue.prevsent.dev.tsv", dtype = np.dtype(str), delimiter = "\n")
    prevsent_dev = [x.split() for x in prevsent_dev]
    prevsent_test = np.loadtxt(data_dir + "31210-s19-hw1/bobsue.prevsent.test.tsv", dtype = np.dtype(str), delimiter = "\n")
    prevsent_test = [x.split() for x in prevsent_test]
    prevsent_train = np.loadtxt(data_dir + "31210-s19-hw1/bobsue.prevsent.train.tsv", dtype = np.dtype(str), delimiter = "\n")
    prevsent_train = [x.split() for x in prevsent_train]

    return(lm_train, lm_dev, lm_test, prevsent_train, prevsent_dev, prevsent_test)

lm_train_raw, lm_dev_raw, lm_test_raw, prevsent_train_raw, prevsent_dev_raw, prevsent_test_raw = read_data()

voc = np.loadtxt(data_dir + "31210-s19-hw1/bobsue.voc.txt", dtype = np.dtype(str), delimiter = "\n")
idx2word = {i:w for i, w in enumerate(voc)} # leave 0 for padding
word2idx = {w:i for i, w in enumerate(voc)}
print('unique words:', len(idx2word))

def confirm_num_pred(data):
    '''check the number of predictions to make,
    stated in HW instruction p2 paragraph 2
    which is the number of unique words except <s> '''
    d = [x for sublist in data for x in sublist if x != '<s>'] # flatten list
    return(len(d))
print(confirm_num_pred(lm_dev_raw)) # 7957
print(confirm_num_pred(lm_test_raw)) # 8059

def tensorize_pad(data):
    '''
    transfor word to tensors and pad to the same length
    6036*21
    '''
    idx_data = [torch.tensor([word2idx[w] for w in sen], dtype = torch.long) for sen in data]
    padded = nn.utils.rnn.pad_sequence(idx_data, batch_first = True, padding_value = len(word2idx))
    if torch.cuda.is_available():
        padded = padded.cuda()
    return(padded)
lm_train = tensorize_pad(lm_train_raw)
lm_dev = tensorize_pad(lm_dev_raw)
lm_test = tensorize_pad(lm_test_raw)

prevsent_train = tensorize_pad(prevsent_train_raw)
prevsent_dev = tensorize_pad(prevsent_dev_raw)
prevsent_test = tensorize_pad(prevsent_test_raw)

print("lm_train:", lm_train.shape)
print("lm_dev:", lm_dev.shape)
print("lm_test", lm_test.shape)
print("prevsent_train", prevsent_train.shape)
print("prevsent_dev", prevsent_dev.shape)
print("prevsent_test", prevsent_test.shape)

"""### 1.0.2 Helper functions"""

def print_list(input_list, elem_type = "int"):
    out = ""
    if elem_type != "int":
        input_list = list(input_list.cpu().numpy())
    for s in input_list:
        if s == len(word2idx):
            break
        out += idx2word[s] + " "
    return(out)

def plot_epoch_losses_acc(epoch_train_losses, epoch_train_acc, epoch_test_losses, epoch_test_acc):
    plt.plot(range(len(epoch_train_losses)), epoch_train_losses)
    plt.plot(range(len(epoch_test_losses)), epoch_test_losses)
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(('train', 'dev'))
    plt.title('Loss by Epoch')
    plt.show()

    plt.plot(range(len(epoch_train_acc)), epoch_train_acc)
    plt.plot(range(len(epoch_test_acc)), epoch_test_acc)
    plt.ylabel('Acc')
    plt.xlabel('Epoch')
    plt.legend(('train', 'dev'))
    plt.title('Accuracy by Epoch')
    plt.show()
    return(0)

class Error_Record_Method():
    def __init__(self):
        self.errors = {}
    def add(self, true, pred):
        if true != pred:
            error = (true, pred)
            if error in self.errors:
                self.errors[error] += 1
            else:
                self.errors[error] = 1

    def get_error(self, input_type, input_word, num):
        if input_type == 'true':
            temp_errors = {key:value for key, value in self.errors.items() if key[0] == input_word}
        elif input_type == 'pred':
            temp_errors = {key:value for key, value in self.errors.items() if key[1] == input_word}
        
        sorted_errors = sorted(temp_errors, key = temp_errors.get, reverse = True)
        print('{:5} |({:10}, {:10})| {:5}'.format('Order', 'True', 'Pred', 'count'))
        for i in range(min(num, len(sorted_errors))):
            print('{:5} |({:10}, {:10})| {:5}'.format(i+1, sorted_errors[i][0], sorted_errors[i][1], temp_errors[sorted_errors[i]]))
        
    def print_freq_error(self, num):
        sorted_errors = sorted(self.errors, key = self.errors.get, reverse = True)
        print('{:5} |({:10}, {:10})| {:5}'.format('Order', 'True', 'Pred', 'count'))
        for i in range(min(num, len(sorted_errors))):
            print('{:5} |({:10}, {:10})| {:5}'.format(i+1, sorted_errors[i][0], sorted_errors[i][1], self.errors[sorted_errors[i]]))

"""### 1.0.3 MyLSTM"""

class MyLSTM(nn.Module):
    '''
    input 1 dim tensor (e.g. 0, means <s>)
    output 1498 dim tensor
    '''
    def __init__(self, embedding_dim, hidden_dim, vocab_size):
        super(MyLSTM, self).__init__()
        self.hidden_dim = hidden_dim
                
        self.emb_i = nn.Embedding(vocab_size, embedding_dim)
        self.decode = nn.Linear(hidden_dim, vocab_size, bias = False) # like the reverse of nn.Embedding
        
        self.lstm = nn.LSTM(embedding_dim, hidden_dim)

    def forward(self, x, hidden_last):
        '''
        x: word by word
        the most original design
        '''
        embedded = self.emb_i(x) # 1 -> 1*200
        out, hidden = self.lstm(embedded.unsqueeze(0), hidden_last) # 1*1*200 (seq_len*batch_size*embedding_size) -> 1*1*200
        decoded = self.decode(out[-1]) # 1*200 -> 1*1498
        
        self.hidden = hidden # save the hidden state for calc bin loss
        
        return(decoded, hidden) # return hiddens for the next

    def calc_bin_loss(self, target, neg):
        '''
        emb_o:  1*1498 -> 1*200
        decode: 1*200  -> 1*1498
        '''        
        sum_neg_loss = 0
        for neg0 in neg:
            neg_tens = hot_code(word2idx[neg0]) # 1*1498
            emb_o_y = torch.matmul(neg_tens, self.decode.weight) # 1*1498 * 1498*200 = 1*200
            score_neg0 = torch.dot(emb_o_y[0], self.hidden[0][0, 0]) # 200 dot 200
            sum_neg_loss += torch.log(1 - torch.sigmoid(score_neg0) + hparams['epsilon']) # epsilon to avoid log(0) = inf
            # if score is big,   sigmoid(score) = 1, log(0 + 1e-10) = -23
            # if score is small, sigmoid(score) = 0, log(1 + 1e-10) = 1e-10
            # this shouldn't be nan
        
        target_tens = hot_code(target)
        emb_o_y = torch.matmul(target_tens, self.decode.weight) # = 1*200
        score_y_t = torch.dot(emb_o_y[0], self.hidden[0][0, 0])
        loss = -torch.log(torch.sigmoid(score_y_t) + hparams['epsilon']) - sum_neg_loss/len(neg)        
        return(loss)
    
    def calc_chl(self, target, neg):
        '''
        contrastive hinge loss
        '''
        target_tens = hot_code(target)
        emb_o_y = torch.matmul(target_tens, self.decode.weight) # = 1*200
        score_y_t = torch.dot(emb_o_y[0], self.hidden[0][0, 0])
        
        sum_max = 0
        for neg0 in neg:
            neg_tens = hot_code(word2idx[neg0]) # 1*1498
            emb_o_y = torch.matmul(neg_tens, self.decode.weight) # 1*1498 * 1498*200 = 1*200
            score_neg0 = torch.dot(emb_o_y[0], self.hidden[0][0, 0]) # 200 dot 200
            
            zero = torch.zeros(1)
            if torch.cuda.is_available():
                zero = zero.cuda()
            sum_max += torch.max(zero, 1 - score_y_t + score_neg0)
        return(sum_max)

    def init_hidden(self):
        output1 = torch.zeros(1, 1, self.hidden_dim)
        output2 = torch.zeros(1, 1, self.hidden_dim)
        if torch.cuda.is_available():
            output1 = output1.cuda()
            output2 = output2.cuda()
        return(output1, output2)

"""### 1.0.4 train and evallm function"""

def train():
    '''
    1. train on lm_train
       using lm_dev for early stopping,
       get best model by best accuracy on lm_dev
    2. train again using best epoch
       use best model and report accuracy on lm_test
    '''
    # initialize model
    if hparams['rand_seed'] != 0:
        torch.manual_seed(hparams['rand_seed'])
    global model
    model = MyLSTM(embedding_dim = 200, hidden_dim = 200, vocab_size = len(word2idx))
    if torch.cuda.is_available() and hparams['gpu'] == True:
        model = model.cuda()
    if hparams['loss_type'] != 'cross_entropy':
        neg = gen_neg(hparams['r'], hparams['f'], hparams['rand_seed'])
    optimizer = optim.Adam(model.parameters(), lr = hparams['learning_rate'])     

    # prepare data
    train_data = hparams['train_data']
    test_data = hparams['test_data']
    if hparams['gpu'] == False: # and if data is on cuda
        train_data = train_data.cpu()
        test_data = test_data.cpu()
        
    # start training
    start = time.time()
    epoch_train_losses, epoch_train_acc = [], []
    epoch_test_losses, epoch_test_acc = [], []
    error_records = Error_Record_Method()
    example_pred = '\n'
    for e in range(hparams['epochs']):
        # train
        train_losses = []
        train_acc_right, train_acc_total = 0, 0
        for i in range(hparams['train_size']): # every sentence
            pred = [int(train_data[i, 0])]
            loss_sen = 0
            sen_acc_right, sen_acc_total = 0, 0
            hidden_last = model.init_hidden()
            for j in range(train_data.shape[1]-1): # every word
                if train_data[i, j+1] == len(word2idx): # 1498, should stop
                    break
                model.zero_grad()
                
                pred_tens, hidden_last = model(train_data[i, j].unsqueeze(0), hidden_last)
                
                if hparams['loss_type'] == 'cross_entropy':
                    target_tens = train_data[i, j+1].unsqueeze(0)
                    loss_sen += F.cross_entropy(pred_tens, target_tens)
                elif hparams['loss_type'] == 'bin_log_loss_neg_sampling':
                    loss_sen += model.calc_bin_loss(int(train_data[i, j+1]), neg)
                elif hparams['loss_type'] == 'contrastive_hinge_loss':
                    loss_sen += model.calc_chl(int(train_data[i, j+1]), neg)
                
                pred_idx = int(torch.argmax(pred_tens, dim = 1))
                target_idx = int(train_data[i, j+1])
                pred.append(pred_idx)                
                sen_acc_total += 1
                sen_acc_right += 1 if pred_idx == target_idx else 0
                
                error_records.add(idx2word[target_idx], idx2word[pred_idx])
                
            # back prop loss per sentence
            loss_sen.backward()
            optimizer.step()
            train_losses.append(loss_sen.item())
            train_acc_right += sen_acc_right
            train_acc_total += sen_acc_total
            
            if hparams['print_pred'] != 0:
                if i % round(hparams['train_size'] / hparams['print_pred']) == 0:
                    example_pred += 'Epoch: '+str(e)+' | training: '+str(i)+' | sen_acc: '+str(round(sen_acc_right/sen_acc_total, 3))+'\n    predicted: '+str(print_list(pred))+'\n    actual:    '+str(print_list(train_data[i], elem_type = "tens"))+'\n'
        epoch_train_losses.append(np.mean(train_losses))
        epoch_train_acc.append(train_acc_right/train_acc_total)

        # test
        test_losses = []
        test_acc_right, test_acc_total = 0, 0
        for i in range(hparams['test_size']): # every sentence
            pred = [int(test_data[i, 0])]
            loss_sen = 0
            sen_acc_right, sen_acc_total = 0, 0
            hidden_last = model.init_hidden()
            for j in range(test_data.shape[1]-1): # every word
                if test_data[i, j+1] == len(word2idx):
                    break
                                    
                pred_tens, hidden_last = model(test_data[i, j].unsqueeze(0), hidden_last)
                
                if hparams['loss_type'] == 'cross_entropy':
                    target_tens = test_data[i, j+1].unsqueeze(0)
                    loss_sen += F.cross_entropy(pred_tens, target_tens)
                elif hparams['loss_type'] == 'bin_log_loss_neg_sampling':
                    loss_sen += model.calc_bin_loss(int(test_data[i, j+1]), neg)
                elif hparams['loss_type'] == 'contrastive_hinge_loss':
                    loss_sen += model.calc_chl(int(test_data[i, j+1]), neg)
                    
                pred_idx = int(torch.argmax(pred_tens, dim = 1))
                target_idx = int(test_data[i, j+1])
                pred.append(pred_idx)
                sen_acc_total += 1
                sen_acc_right += 1 if pred_idx == target_idx else 0
                
                error_records.add(idx2word[target_idx], idx2word[pred_idx])
            
            # do not back prop
            test_losses.append(loss_sen.item())
            test_acc_right += sen_acc_right
            test_acc_total += sen_acc_total
            
            if hparams['print_pred'] != 0:
                if i % round(hparams['test_size'] / hparams['print_pred']) == 0:
                    example_pred += 'Epoch: '+str(e)+' | testng: '+str(i)+' | sen_acc: '+str(round(sen_acc_right/sen_acc_total, 3))+'\n    predicted: '+str(print_list(pred))+'\n    actual:    '+str(print_list(test_data[i], elem_type = "tens"))+'\n'
        
        epoch_test_losses.append(np.mean(test_losses))
        epoch_test_acc.append(test_acc_right/test_acc_total)

        if e == 0:
            print('Epoch|TrainLoss  Acc  |TestLoss  Acc  |RunTime')
        if e % hparams['logint'] == 0:
            print('{:>5}|{:9.3f}  {:3.3f}|{:8.3f}  {:3.3f}|{:7.1f}'
                  .format(e, epoch_train_losses[-1], epoch_train_acc[-1], 
                          epoch_test_losses[-1], epoch_test_acc[-1], time.time()-start))
    
    # output
    print(example_pred)
    if hparams['plot']:
        plot_epoch_losses_acc(epoch_train_losses, epoch_train_acc,
                              epoch_test_losses, epoch_test_acc)
    return(error_records, epoch_test_acc)

def evallm():
    print('training for early stopping...')
    _, epoch_dev_acc = train()
    best_num_epoch = np.argmax(epoch_dev_acc)+1
    print()

    # retrain model using the best num of epochs from above, test on lm_test
    print('training again till best number of epoch:', best_num_epoch)
    hparams['test_data'] = lm_test
    store_previous_epoch_num = hparams['epochs']
    store_previous_plot = hparams['plot']
    hparams['epochs'] = best_num_epoch
    hparams['plot'] = False
    
    error_records_test, epoch_test_acc = train()
    
    hparams['epochs'] = store_previous_epoch_num
    hparams['plot'] = store_previous_plot
    
    print('final test acc:', round(epoch_test_acc[-1], 3))
    return(error_records_test)

"""## 1.1 EVALLM

### 1.1.1 Training printout

#### 1.1.1.1 experiment with learning rate
"""

hparams = {
    'learning_rate': 0.01,
    'epochs':        10,
    'train_size':    6036, # max 6036
    'test_size':     750,  # max 750
    'logint':        1,   # frequency of printing result every epoch
    'print_pred':    0,   # frequency of printing sentence in one epoch, 0 don't print
    'train_data':    lm_train,
    'test_data':     lm_dev,
    'plot':          True,
    'rand_seed':     555,
    'gpu':           True,
    'loss_type':     'cross_entropy'}
error_records = evallm()

hparams = {
    'learning_rate': 0.001,
    'epochs':        10,
    'train_size':    1000, # max 6036
    'test_size':     100,  # max 750
    'logint':        1,   # frequency of printing result every epoch
    'print_pred':    0,   # frequency of printing sentence in one epoch, 0 don't print
    'train_data':    lm_train,
    'test_data':     lm_dev,
    'plot':          True,
    'rand_seed':     555,
    'gpu':           True,
    'loss_type':     'cross_entropy'}
error_records = evallm()

hparams = {
    'learning_rate': 0.0001,
    'epochs':        10,
    'train_size':    1000, # max 6036
    'test_size':     100,  # max 750
    'logint':        1,   # frequency of printing result every epoch
    'print_pred':    0,   # frequency of printing sentence in one epoch, 0 don't print
    'train_data':    lm_train,
    'test_data':     lm_dev,
    'plot':          True,
    'rand_seed':     555,
    'gpu':           True,
    'loss_type':     'cross_entropy'}
error_records = evallm()

hparams = {
    'learning_rate': 0.0005,
    'epochs':        20,
    'train_size':    1000, # max 6036
    'test_size':     100,  # max 750
    'logint':        1,   # frequency of printing result every epoch
    'print_pred':    0,   # frequency of printing sentence in one epoch, 0 don't print
    'train_data':    lm_train,
    'test_data':     lm_dev,
    'plot':          True,
    'rand_seed':     555,
    'gpu':           True,
    'loss_type':     'cross_entropy'}
error_records = evallm()

hparams = {
    'learning_rate': 0.0005,
    'epochs':        10,
    'train_size':    6036, # max 6036
    'test_size':     750,  # max 750
    'logint':        1,   # frequency of printing result every epoch
    'print_pred':    0,   # frequency of printing sentence in one epoch, 0 don't print
    'train_data':    lm_train,
    'test_data':     lm_dev,
    'plot':          True,
    'rand_seed':     555,
    'gpu':           True,
    'loss_type':     'cross_entropy'}
error_records = evallm()

"""#### 1.1.1.2 final model"""

hparams = {
    'learning_rate': 0.001,
    'epochs':        15,
    'train_size':    6036, # max 6036
    'test_size':     750,  # max 750
    'logint':        1,   # frequency of printing result every epoch
    'print_pred':    0,   # frequency of printing sentence in one epoch, 0 don't print
    'train_data':    lm_train,
    'test_data':     lm_dev,
    'plot':          True,
    'rand_seed':     555,
    'gpu':           True,
    'loss_type':     'cross_entropy'}
error_records = evallm()

"""### 1.1.2 Description

For optimizer I use Adam, I adjust only the learning rate and keep others as default. (See [torch.optim — PyTorch master documentation](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) for default values.)

I used training size of 1000 and test size of 100 to experiment with different learning rates, these trivial results are in "1.1.1.1"

- lr = 0.001, acc = 0.295
- lr = 0.0005, acc = 0.286
- lr = 0.0001, acc = 0.284

the highest accuracy achieved is with lr = 0.001.

In the Final model, following the EVALLM paradigm, I achieved a 0.332 acc on dev set in the 2nd epoch. And then have 0.328 acc on test set.

PS. If use the whole dataset to test,

- lr = 0.01, acc = 0.266
- lr = 0.001, acc = 0.328
- lr = 0.0005, acc = 0.331

lr = 0.0005 is better but I found it too late.

# 2 Error Analysis

## 2.1 print errors

I implemented `Error_Record_Method` in [1.0.2 Helper functions](https://https://colab.research.google.com/drive/12OX4cagImWojguHwmaiH2CXiBihhhHY2#scrollTo=HwcbODtzfxG9) that is able to record record errors, calculate frequency and print the required error with given word. 

Example as below, all error that the truth is 'She' is printed, in the order of count.
"""

error_records.get_error('true', 'She', 35)

"""## 2.2 Top 35 errors

Top 35 errors are printed below, ranked by their counts.
"""

error_records.print_freq_error(35)

"""## 2.3 Error categories

I categorized as the following groups

- Subjects such as (He, Bob)
    - 2, 4, 5, 7, 8, 9, 25, 26, 35
- possibly ending point in a sentence such as (and, .)
    - 1, 3, 13, 15, 16, 18, 23, 30, 32, 34
- verbs and tense such as (got, was)
    - 6, 10, 19, 29, 31
- 's, his, her, the, a
    - 11, 12, 14, 27, 28
    
The model is oftern wrong for common words. Therefore more attention should be put to the common words compared to rare words.

The model is good in selecting out possible words because these errors are not very ridiculous and sometimes are interchangable. However it is hard for a machine to accurately select the correct word.

# 3 Binary log loss

## 3.1 Code

All essential codes are implemented in the `MyLSTM` class. Below are just some needed helper functions.
"""

def calc_unig():
    count = {w:0 for w in word2idx.keys() if w != '<s>'}
    for sen in lm_train_raw:
        for w in sen:
            if w != '<s>':
                count[w] += 1
    total = sum(count.values())
    return({w:v/total for w, v in count.items()})
UNIG = calc_unig()

def gen_neg(r, f, seed):
    '''
    generate UNIG-f as described by the description,
    return r words, with f-flattened unigram probability
    when f = 0, its the same as UNIF
    '''
    if seed != None:
        np.random.seed(seed)
    p = np.array(list((UNIG.values())))**f
    p /= sum(p)
    return(np.random.choice(list(UNIG.keys()), r, replace = False, p = p))

def hot_code(y):
    '''
    int -> 1*1498
    '''
    y_hot_code = torch.zeros(1, 1498)
    if torch.cuda.is_available():
        y_hot_code = y_hot_code.cuda()
    y_hot_code[0, int(y)] = 1
    return(y_hot_code)

"""## 3.2 EVALLM on UNIF

### 3.2.1 Training printout

#### 3.2.1.1 $r = 20$
"""

hparams = {
    'learning_rate': 0.01,
    'epochs':        20,
    'train_size':    100, # max 6036
    'test_size':     10,  # max 750
    'logint':        1, 
    'print_pred':    0,
    'train_data':    lm_train,
    'test_data':     lm_dev,
    'plot':          True,
    'rand_seed':     555,
    'gpu':           True,
    'loss_type':     'bin_log_loss_neg_sampling',
    'r':             20,
    'f':             0,
    'epsilon':       1e-5}
_ = evallm()

"""#### 3.2.1.2 $r = 100$"""

hparams['r'] = 100
_ = evallm()

"""#### 3.2.1.3 $r = 500$"""

hparams['r'] = 500
_ = evallm()

"""### 3.2.2 description

From the above results, I get

- r = 20, acc = 0.024
- r = 100, acc = 0.137
- r = 500, acc = 0.145

from using a small dataset because of the constraint of time.

This suggest the performance is better with higher $r$. It can be a lot more higher if using the whole dataset and with lower learning rate. However, the training time increases significantly with higher $r$ so this is a trade-off that needed to be considered.

## 3.3 EVALLM on UNIG

### 3.3.1 Training printout

#### 3.3.1.1 $f = 1/2$
"""

hparams = {
    'learning_rate': 0.001,
    'epochs':        20,
    'train_size':    100, # max 6036
    'test_size':     10,  # max 750
    'logint':        1, 
    'print_pred':    0,
    'train_data':    lm_train,
    'test_data':     lm_dev,
    'plot':          True,
    'rand_seed':     555,
    'gpu':           True,
    'loss_type':     'bin_log_loss_neg_sampling',
    'r':             20,
    'f':             1/2,
    'epsilon':       1e-5}
_ = evallm()

"""#### 3.3.1.2 $f = 1$"""

hparams['f'] = 1
_ = evallm()

"""#### 3.3.1.3 $f = 3/4$"""

hparams['f'] = 3/4
_ = evallm()

"""### 3.3.2 description

From the above results, using $r = 20$ I get

- f = 1/2, acc = 0.161
- f = 1, acc = 0.048
- f = 3/4, acc = 0.004

Comparuing to $f = 0$ from the last section, the accuracy was 0.024. Either $f = 1/2$ or $f = 1$ is better than that.

A better $f$ could be around 1/2. But more experiments and dataset is needed.

## 3.4 Efficiency

I comparing three:

- log loss using whole dataset, in "1.1.1.2"
- log loss using 1/6 of dataset, in appendix, more fair compare to the below one
- binary log loss with negative sampling using 1/6 of dataset with $r = 20$ and $f = 1/2$, in "3.3.1.1"

the accuracy of them are

- 0.328
- 0.218
- 0.161

- #sents/secs
    - log loss: 52.28
    - log loss, 1/6: 79.14, **better**
    - binary log loss: 5.75
- #sents for max acc
    - log loss: 12072
    - log loss, 1/6: 700, **better**
    - binary log loss: 1200
- time for max acc
    - log loss: 257.5
    - log loss, 1/6: 9, **better**
    - binary log loss: 142.2
    
We can see that the binary log loss is a very slow method judging by the sentences processed per second, and it need more sentences to achieve the max acc. Also, it needs more time for max acc. However, it is possible to tune the parameters well and give it more time so that the binary log loss can have a higher accuracy.



---


**For part 4-5, please go [here](https://colab.research.google.com/drive/1dpFW7cimQi72p_XGpOVZMDBbOQCOYXJp).**

# Appendix

Please ignore these trivial training results that from my own experiments.
"""

hparams = {
    'learning_rate': 0.001,
    'epochs':        20,
    'train_size':    100, # max 6036
    'test_size':     10,  # max 750
    'logint':        1,   # frequency of printing result every epoch
    'print_pred':    0,   # frequency of printing sentence in one epoch, 0 don't print
    'train_data':    lm_train,
    'test_data':     lm_dev,
    'plot':          True,
    'rand_seed':     555,
    'gpu':           True,
    'loss_type':     'cross_entropy'}
error_records = evallm()

hparams = {
    'learning_rate': 0.01,
    'epochs':        20,
    'train_size':    1000, # max 6036
    'test_size':     10,  # max 750
    'logint':        1, 
    'print_pred':    0,
    'train_data':    lm_train,
    'test_data':     lm_dev,
    'plot':          True,
    'rand_seed':     555,
    'gpu':           True,
    'loss_type':     'bin_log_loss_neg_sampling',
    'r':             20,
    'f':             0,
    'epsilon':       1e-5}
er = evallm()

hparams['r'] = 100
_ = evallm()