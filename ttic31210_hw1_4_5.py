# -*- coding: utf-8 -*-
"""TTIC31210_HW1_4-5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dpFW7cimQi72p_XGpOVZMDBbOQCOYXJp

# TTIC 31210 HW1 Language Modeling Part 2/2

**Name: Yi Hung Liu**

- UCID: yhliu
- Email: yhliu@uchicago.edu


---


This is the second part, section 4-5, of the assignment. Please skip to "4 Larger context". Where the skipped part are just essential codes copied from part 1.

# 1 Log Loss

## 1.0 Code

### 1.0.1 Loading data and preprocessing
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import csv
import string
import time
import matplotlib.pyplot as plt

FROM_GOOGLE_DRIVE = True
if FROM_GOOGLE_DRIVE:
    from google.colab import drive
    drive.mount('/content/drive')
    drive_dir = '/content/drive/My Drive/'
else:
    drive_dir = '/Users/T/Google Drive/'
data_dir = drive_dir + '02 UChicago/3 19Spring/TTIC 31210 Advanced NLP/HW1/'

def read_data():
    '''load and split data'''
    lm_dev = []
    lm_test = []
    lm_train = []
    
    lm_dev = np.loadtxt(data_dir + "31210-s19-hw1/bobsue.lm.dev.txt", dtype = np.dtype(str), delimiter = "\n")
    lm_dev = [x.split() for x in lm_dev]
    lm_test = np.loadtxt(data_dir + "31210-s19-hw1/bobsue.lm.test.txt", dtype = np.dtype(str), delimiter = "\n")
    lm_test = [x.split() for x in lm_test]
    lm_train = np.loadtxt(data_dir + "31210-s19-hw1/bobsue.lm.train.txt", dtype = np.dtype(str), delimiter = "\n")
    lm_train = [x.split() for x in lm_train]

    prevsent_dev = []
    prevsent_test = []
    prevsent_train = []

    prevsent_dev = np.loadtxt(data_dir + "31210-s19-hw1/bobsue.prevsent.dev.tsv", dtype = np.dtype(str), delimiter = "\n")
    prevsent_dev = [x.split() for x in prevsent_dev]
    prevsent_test = np.loadtxt(data_dir + "31210-s19-hw1/bobsue.prevsent.test.tsv", dtype = np.dtype(str), delimiter = "\n")
    prevsent_test = [x.split() for x in prevsent_test]
    prevsent_train = np.loadtxt(data_dir + "31210-s19-hw1/bobsue.prevsent.train.tsv", dtype = np.dtype(str), delimiter = "\n")
    prevsent_train = [x.split() for x in prevsent_train]

    return(lm_train, lm_dev, lm_test, prevsent_train, prevsent_dev, prevsent_test)

lm_train_raw, lm_dev_raw, lm_test_raw, prevsent_train_raw, prevsent_dev_raw, prevsent_test_raw = read_data()

voc = np.loadtxt(data_dir + "31210-s19-hw1/bobsue.voc.txt", dtype = np.dtype(str), delimiter = "\n")
idx2word = {i:w for i, w in enumerate(voc)} # leave 0 for padding
word2idx = {w:i for i, w in enumerate(voc)}
print('unique words:', len(idx2word))

def confirm_num_pred(data):
    '''check the number of predictions to make,
    stated in HW instruction p2 paragraph 2
    which is the number of unique words except <s> '''
    d = [x for sublist in data for x in sublist if x != '<s>'] # flatten list
    return(len(d))
print(confirm_num_pred(lm_dev_raw)) # 7957
print(confirm_num_pred(lm_test_raw)) # 8059

def tensorize_pad(data):
    '''
    transfor word to tensors and pad to the same length
    6036*21
    '''
    idx_data = [torch.tensor([word2idx[w] for w in sen], dtype = torch.long) for sen in data]
    padded = nn.utils.rnn.pad_sequence(idx_data, batch_first = True, padding_value = len(word2idx))
    if torch.cuda.is_available():
        padded = padded.cuda()
    return(padded)
lm_train = tensorize_pad(lm_train_raw)
lm_dev = tensorize_pad(lm_dev_raw)
lm_test = tensorize_pad(lm_test_raw)

prevsent_train = tensorize_pad(prevsent_train_raw)
prevsent_dev = tensorize_pad(prevsent_dev_raw)
prevsent_test = tensorize_pad(prevsent_test_raw)

print("lm_train:", lm_train.shape)
print("lm_dev:", lm_dev.shape)
print("lm_test", lm_test.shape)
print("prevsent_train", prevsent_train.shape)
print("prevsent_dev", prevsent_dev.shape)
print("prevsent_test", prevsent_test.shape)

"""### 1.0.2 Helper functions"""

def print_list(input_list, elem_type = "int"):
    out = ""
    if elem_type != "int":
        input_list = list(input_list.cpu().numpy())
    for s in input_list:
        if s == len(word2idx):
            break
        out += idx2word[s] + " "
    return(out)

def plot_epoch_losses_acc(epoch_train_losses, epoch_train_acc, epoch_test_losses, epoch_test_acc):
    plt.plot(range(len(epoch_train_losses)), epoch_train_losses)
    plt.plot(range(len(epoch_test_losses)), epoch_test_losses)
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(('train', 'dev'))
    plt.title('Loss by Epoch')
    plt.show()

    plt.plot(range(len(epoch_train_acc)), epoch_train_acc)
    plt.plot(range(len(epoch_test_acc)), epoch_test_acc)
    plt.ylabel('Acc')
    plt.xlabel('Epoch')
    plt.legend(('train', 'dev'))
    plt.title('Accuracy by Epoch')
    plt.show()
    return(0)

class Error_Record_Method():
    def __init__(self):
        self.errors = {}
    def add(self, true, pred):
        if true != pred:
            error = (true, pred)
            if error in self.errors:
                self.errors[error] += 1
            else:
                self.errors[error] = 1

    def get_error(self, input_type, input_word, num):
        if input_type == 'true':
            temp_errors = {key:value for key, value in self.errors.items() if key[0] == input_word}
        elif input_type == 'pred':
            temp_errors = {key:value for key, value in self.errors.items() if key[1] == input_word}
        
        sorted_errors = sorted(temp_errors, key = temp_errors.get, reverse = True)
        print('{:5} |({:10}, {:10})| {:5}'.format('Order', 'True', 'Pred', 'count'))
        for i in range(min(num, len(sorted_errors))):
            print('{:5} |({:10}, {:10})| {:5}'.format(i+1, sorted_errors[i][0], sorted_errors[i][1], temp_errors[sorted_errors[i]]))
        
    def print_freq_error(self, num):
        sorted_errors = sorted(self.errors, key = self.errors.get, reverse = True)
        print('{:5} |({:10}, {:10})| {:5}'.format('Order', 'True', 'Pred', 'count'))
        for i in range(min(num, len(sorted_errors))):
            print('{:5} |({:10}, {:10})| {:5}'.format(i+1, sorted_errors[i][0], sorted_errors[i][1], self.errors[sorted_errors[i]]))

"""### 1.0.3 MyLSTM"""

class MyLSTM(nn.Module):
    '''
    input 1 dim tensor (e.g. 0, means <s>)
    output 1498 dim tensor
    '''
    def __init__(self, embedding_dim, hidden_dim, vocab_size):
        super(MyLSTM, self).__init__()
        self.hidden_dim = hidden_dim
                
        self.emb_i = nn.Embedding(vocab_size, embedding_dim)
        self.decode = nn.Linear(hidden_dim, vocab_size, bias = False) # like the reverse of nn.Embedding
        
        self.lstm = nn.LSTM(embedding_dim, hidden_dim)

    def forward(self, x, hidden_last):
        '''
        x: word by word
        the most original design
        '''
        embedded = self.emb_i(x) # 1 -> 1*200
        out, hidden = self.lstm(embedded.unsqueeze(0), hidden_last) # 1*1*200 (seq_len*batch_size*embedding_size) -> 1*1*200
        decoded = self.decode(out[-1]) # 1*200 -> 1*1498
        
        self.hidden = hidden # save the hidden state for calc bin loss
        
        return(decoded, hidden) # return hiddens for the next

    def calc_bin_loss(self, target, neg):
        '''
        emb_o:  1*1498 -> 1*200
        decode: 1*200  -> 1*1498
        '''        
        sum_neg_loss = 0
        for neg0 in neg:
            neg_tens = hot_code(word2idx[neg0]) # 1*1498
            emb_o_y = torch.matmul(neg_tens, self.decode.weight) # 1*1498 * 1498*200 = 1*200
            score_neg0 = torch.dot(emb_o_y[0], self.hidden[0][0, 0]) # 200 dot 200
            sum_neg_loss += torch.log(1 - torch.sigmoid(score_neg0) + hparams['epsilon']) # epsilon to avoid log(0) = inf
            # if score is big,   sigmoid(score) = 1, log(0 + 1e-10) = -23
            # if score is small, sigmoid(score) = 0, log(1 + 1e-10) = 1e-10
            # this shouldn't be nan
        
        target_tens = hot_code(target)
        emb_o_y = torch.matmul(target_tens, self.decode.weight) # = 1*200
        score_y_t = torch.dot(emb_o_y[0], self.hidden[0][0, 0])
        loss = -torch.log(torch.sigmoid(score_y_t) + hparams['epsilon']) - sum_neg_loss/len(neg)        
        return(loss)
    
    def calc_chl(self, target, neg):
        '''
        contrastive hinge loss
        '''
        target_tens = hot_code(target)
        emb_o_y = torch.matmul(target_tens, self.decode.weight) # = 1*200
        score_y_t = torch.dot(emb_o_y[0], self.hidden[0][0, 0])
        
        sum_max = 0
        for neg0 in neg:
            neg_tens = hot_code(word2idx[neg0]) # 1*1498
            emb_o_y = torch.matmul(neg_tens, self.decode.weight) # 1*1498 * 1498*200 = 1*200
            score_neg0 = torch.dot(emb_o_y[0], self.hidden[0][0, 0]) # 200 dot 200
            
            zero = torch.zeros(1)
            if torch.cuda.is_available():
                zero = zero.cuda()
            sum_max += torch.max(zero, 1 - score_y_t + score_neg0)
        return(sum_max)

    def init_hidden(self):
        output1 = torch.zeros(1, 1, self.hidden_dim)
        output2 = torch.zeros(1, 1, self.hidden_dim)
        if torch.cuda.is_available():
            output1 = output1.cuda()
            output2 = output2.cuda()
        return(output1, output2)

"""### 1.0.4 train and evallm function"""

def train():
    '''
    1. train on lm_train
       using lm_dev for early stopping,
       get best model by best accuracy on lm_dev
    2. train again using best epoch
       use best model and report accuracy on lm_test
    '''
    # initialize model
    if hparams['rand_seed'] != 0:
        torch.manual_seed(hparams['rand_seed'])
    global model
    model = MyLSTM(embedding_dim = 200, hidden_dim = 200, vocab_size = len(word2idx))
    if torch.cuda.is_available() and hparams['gpu'] == True:
        model = model.cuda()
    if hparams['loss_type'] != 'cross_entropy':
        neg = gen_neg(hparams['r'], hparams['f'], hparams['rand_seed'])
    optimizer = optim.Adam(model.parameters(), lr = hparams['learning_rate'])     

    # prepare data
    train_data = hparams['train_data']
    test_data = hparams['test_data']
    if hparams['gpu'] == False: # and if data is on cuda
        train_data = train_data.cpu()
        test_data = test_data.cpu()
        
    # start training
    start = time.time()
    epoch_train_losses, epoch_train_acc = [], []
    epoch_test_losses, epoch_test_acc = [], []
    error_records = Error_Record_Method()
    example_pred = '\n'
    for e in range(hparams['epochs']):
        # train
        train_losses = []
        train_acc_right, train_acc_total = 0, 0
        for i in range(hparams['train_size']): # every sentence
            pred = [int(train_data[i, 0])]
            loss_sen = 0
            sen_acc_right, sen_acc_total = 0, 0
            hidden_last = model.init_hidden()
            for j in range(train_data.shape[1]-1): # every word
                if train_data[i, j+1] == len(word2idx): # 1498, should stop
                    break
                model.zero_grad()
                
                pred_tens, hidden_last = model(train_data[i, j].unsqueeze(0), hidden_last)
                
                if hparams['loss_type'] == 'cross_entropy':
                    target_tens = train_data[i, j+1].unsqueeze(0)
                    loss_sen += F.cross_entropy(pred_tens, target_tens)
                elif hparams['loss_type'] == 'bin_log_loss_neg_sampling':
                    loss_sen += model.calc_bin_loss(int(train_data[i, j+1]), neg)
                elif hparams['loss_type'] == 'contrastive_hinge_loss':
                    loss_sen += model.calc_chl(int(train_data[i, j+1]), neg)
                
                pred_idx = int(torch.argmax(pred_tens, dim = 1))
                target_idx = int(train_data[i, j+1])
                pred.append(pred_idx)                
                sen_acc_total += 1
                sen_acc_right += 1 if pred_idx == target_idx else 0
                
                error_records.add(idx2word[target_idx], idx2word[pred_idx])
                
            # back prop loss per sentence
            loss_sen.backward()
            optimizer.step()
            train_losses.append(loss_sen.item())
            train_acc_right += sen_acc_right
            train_acc_total += sen_acc_total
            
            if hparams['print_pred'] != 0:
                if i % round(hparams['train_size'] / hparams['print_pred']) == 0:
                    example_pred += 'Epoch: '+str(e)+' | training: '+str(i)+' | sen_acc: '+str(round(sen_acc_right/sen_acc_total, 3))+'\n    predicted: '+str(print_list(pred))+'\n    actual:    '+str(print_list(train_data[i], elem_type = "tens"))+'\n'
        epoch_train_losses.append(np.mean(train_losses))
        epoch_train_acc.append(train_acc_right/train_acc_total)

        # test
        test_losses = []
        test_acc_right, test_acc_total = 0, 0
        for i in range(hparams['test_size']): # every sentence
            pred = [int(test_data[i, 0])]
            loss_sen = 0
            sen_acc_right, sen_acc_total = 0, 0
            hidden_last = model.init_hidden()
            for j in range(test_data.shape[1]-1): # every word
                if test_data[i, j+1] == len(word2idx):
                    break
                                    
                pred_tens, hidden_last = model(test_data[i, j].unsqueeze(0), hidden_last)
                
                if hparams['loss_type'] == 'cross_entropy':
                    target_tens = test_data[i, j+1].unsqueeze(0)
                    loss_sen += F.cross_entropy(pred_tens, target_tens)
                elif hparams['loss_type'] == 'bin_log_loss_neg_sampling':
                    loss_sen += model.calc_bin_loss(int(test_data[i, j+1]), neg)
                elif hparams['loss_type'] == 'contrastive_hinge_loss':
                    loss_sen += model.calc_chl(int(test_data[i, j+1]), neg)
                    
                pred_idx = int(torch.argmax(pred_tens, dim = 1))
                target_idx = int(test_data[i, j+1])
                pred.append(pred_idx)
                sen_acc_total += 1
                sen_acc_right += 1 if pred_idx == target_idx else 0
                
                error_records.add(idx2word[target_idx], idx2word[pred_idx])
            
            # do not back prop
            test_losses.append(loss_sen.item())
            test_acc_right += sen_acc_right
            test_acc_total += sen_acc_total
            
            if hparams['print_pred'] != 0:
                if i % round(hparams['test_size'] / hparams['print_pred']) == 0:
                    example_pred += 'Epoch: '+str(e)+' | testng: '+str(i)+' | sen_acc: '+str(round(sen_acc_right/sen_acc_total, 3))+'\n    predicted: '+str(print_list(pred))+'\n    actual:    '+str(print_list(test_data[i], elem_type = "tens"))+'\n'
        
        epoch_test_losses.append(np.mean(test_losses))
        epoch_test_acc.append(test_acc_right/test_acc_total)

        if e == 0:
            print('Epoch|TrainLoss  Acc  |TestLoss  Acc  |RunTime')
        if e % hparams['logint'] == 0:
            print('{:>5}|{:9.3f}  {:3.3f}|{:8.3f}  {:3.3f}|{:7.1f}'
                  .format(e, epoch_train_losses[-1], epoch_train_acc[-1], 
                          epoch_test_losses[-1], epoch_test_acc[-1], time.time()-start))
    
    # output
    print(example_pred)
    if hparams['plot']:
        plot_epoch_losses_acc(epoch_train_losses, epoch_train_acc,
                              epoch_test_losses, epoch_test_acc)
    return(error_records, epoch_test_acc)

def evallm():
    print('training for early stopping...')
    _, epoch_dev_acc = train()
    best_num_epoch = np.argmax(epoch_dev_acc)+1
    print()

    # retrain model using the best num of epochs from above, test on lm_test
    print('training again till best number of epoch:', best_num_epoch)
    hparams['test_data'] = lm_test
    store_previous_epoch_num = hparams['epochs']
    store_previous_plot = hparams['plot']
    hparams['epochs'] = best_num_epoch
    hparams['plot'] = False
    
    error_records_test, epoch_test_acc = train()
    
    hparams['epochs'] = store_previous_epoch_num
    hparams['plot'] = store_previous_plot
    
    print('final test acc:', round(epoch_test_acc[-1], 3))
    return(error_records_test)

"""# 3 Binary log loss

## 3.1 Code
"""

def calc_unig():
    count = {w:0 for w in word2idx.keys() if w != '<s>'}
    for sen in lm_train_raw:
        for w in sen:
            if w != '<s>':
                count[w] += 1
    total = sum(count.values())
    return({w:v/total for w, v in count.items()})
UNIG = calc_unig()

def gen_neg(r, f, seed):
    '''
    generate UNIG-f as described by the description,
    return r words, with f-flattened unigram probability
    when f = 0, its the same as UNIF
    '''
    if seed != None:
        np.random.seed(seed)
    p = np.array(list((UNIG.values())))**f
    p /= sum(p)
    return(np.random.choice(list(UNIG.keys()), r, replace = False, p = p))

def hot_code(y):
    '''
    int -> 1*1498
    '''
    y_hot_code = torch.zeros(1, 1498)
    if torch.cuda.is_available():
        y_hot_code = y_hot_code.cuda()
    y_hot_code[0, int(y)] = 1
    return(y_hot_code)

"""# 4 Larger context

## 4.0 Code
"""

def train_2sen():
    '''
    input the first sentence word by word, including <s> and </s>,
    input the second sentence as before
    
    train on prevsent_train, early stopping by prevsent_dev, test on prevsent_test
    '''
    # initialize model
    if hparams['rand_seed'] != 0:
        torch.manual_seed(hparams['rand_seed'])
    global model
    model = MyLSTM(embedding_dim = 200, hidden_dim = 200, vocab_size = len(word2idx))
    if torch.cuda.is_available() and hparams['gpu'] == True:
        model = model.cuda()
    optimizer = optim.Adam(model.parameters(), lr=hparams['learning_rate'])     

    # prepare data
    train_data = hparams['train_data']
    test_data = hparams['test_data']
    if hparams['gpu'] == False: # and if data in on cuda
        train_data = train_data.cpu()
        test_data = test_data.cpu()
        
    # start training
    start = time.time()
    epoch_train_losses, epoch_train_acc = [], []
    epoch_test_losses, epoch_test_acc = [], []
    error_records = Error_Record_Method()
    example_pred = '\n'
    for e in range(hparams['epochs']):
        # train
        train_losses = []
        train_acc_right, train_acc_total = 0, 0
        for i in range(hparams['train_size']): # every sentence
            pred = [int(train_data[i, 0])]
            loss_sen = 0
            sen_acc_right, sen_acc_total = 0, 0
            is_second_sen = False
            hidden_last = model.init_hidden()
            for j in range(train_data.shape[1]-1): # every word
                if train_data[i, j+1] == len(word2idx): # 1498, stop
                    break
                model.zero_grad()
                
                pred_tens, hidden_last = model(train_data[i, j].unsqueeze(0), hidden_last) 

                if is_second_sen:
                    target_tens = train_data[i, j+1].unsqueeze(0)
                    loss_sen += F.cross_entropy(pred_tens, target_tens)
                    
                    pred_idx = int(torch.argmax(pred_tens, dim = 1))
                    target_idx = int(train_data[i, j+1])
                    pred.append(pred_idx)                
                    sen_acc_total += 1
                    sen_acc_right += 1 if pred_idx == target_idx else 0

                    error_records.add(idx2word[target_idx], idx2word[pred_idx])

                if train_data[i, j+1] == word2idx['<s>']: # next is <s>, start calc loss
                    is_second_sen = True
                    
            # back prop loss per sentence
            loss_sen.backward()
            optimizer.step()
            train_losses.append(loss_sen.item())
            train_acc_right += sen_acc_right
            train_acc_total += sen_acc_total
            
            if hparams['print_pred'] != 0:
                if i % round(hparams['train_size'] / hparams['print_pred']) == 0:
                    second_start = int((train_data[i] == word2idx['<s>']).nonzero()[1])
                    example_pred += 'Epoch: '+str(e)+' | training: '+str(i)+' | sen_acc: '+str(round(sen_acc_right/sen_acc_total, 3))+'\n    predicted: '+str(print_list(pred))+'\n    actual:    '+str(print_list(train_data[i, second_start:], elem_type = "tens"))+'\n'
        epoch_train_losses.append(np.mean(train_losses))
        epoch_train_acc.append(train_acc_right/train_acc_total)

        # test
        test_losses = []
        test_acc_right, test_acc_total = 0, 0
        for i in range(hparams['test_size']): # every sentence
            pred = [int(test_data[i, 0])]
            loss_sen = 0
            sen_acc_right, sen_acc_total = 0, 0
            is_second_sen = False
            hidden_last = model.init_hidden()
            for j in range(test_data.shape[1]-1): # every word
                if test_data[i, j+1] == len(word2idx):
                    break
                    
                pred_tens, hidden_last = model(test_data[i, j].unsqueeze(0), hidden_last)

                if is_second_sen:
                    target_tens = test_data[i, j+1].unsqueeze(0)
                    loss_sen += F.cross_entropy(pred_tens, target_tens)

                    pred_idx = int(torch.argmax(pred_tens, dim = 1))
                    target_idx = int(test_data[i, j+1])
                    pred.append(pred_idx)
                    sen_acc_total += 1
                    sen_acc_right += 1 if pred_idx == target_idx else 0

                    error_records.add(idx2word[target_idx], idx2word[pred_idx])
                    
                if test_data[i, j+1] == word2idx['<s>']: # next is <s>, start calc loss
                    is_second_sen = True
                    
            # do not back prop
            test_losses.append(loss_sen.item())
            test_acc_right += sen_acc_right
            test_acc_total += sen_acc_total
            
            if hparams['print_pred'] != 0:
                if i % round(hparams['test_size'] / hparams['print_pred']) == 0:
                    second_start = int((test_data[i] == word2idx['<s>']).nonzero()[1])
                    example_pred += 'Epoch: '+str(e)+' | testng: '+str(i)+' | sen_acc: '+str(round(sen_acc_right/sen_acc_total, 3))+'\n    predicted: '+str(print_list(pred))+'\n    actual:    '+str(print_list(test_data[i, second_start:], elem_type = "tens"))+'\n'
        
        epoch_test_losses.append(np.mean(test_losses))
        epoch_test_acc.append(test_acc_right/test_acc_total)

        if e == 0:
            print('Epoch|TrainLoss  Acc  |TestLoss  Acc  |RunTime')
        if e % hparams['logint'] == 0:
            print('{:>5}|{:9.3f}  {:3.3f}|{:8.3f}  {:3.3f}|{:7.1f}'
                  .format(e, epoch_train_losses[-1], epoch_train_acc[-1], 
                          epoch_test_losses[-1], epoch_test_acc[-1], time.time()-start))
    
    # output
    print(example_pred)
    if hparams['plot']:
        plot_epoch_losses_acc(epoch_train_losses, epoch_train_acc,
                              epoch_test_losses, epoch_test_acc)
    return(error_records, epoch_test_acc)

def evalprev():
    print('training for early stopping...')
    _, epoch_test_acc = train_2sen()
    best_num_epoch = np.argmax(epoch_test_acc)+1
    print()

    # retrain model using the best num of epochs from above, test on lm_test
    print('training again till best number of epoch:', best_num_epoch)
    hparams['test_data'] = prevsent_test
    hparams['epochs'] = best_num_epoch
    hparams['plot'] = False
    error_records_test, epoch_test_acc = train_2sen()
    
    print('final achieved test acc:', round(epoch_test_acc[-1], 3))
    return(error_records_test)

"""## 4.1 EVALPREV

### 4.1.1 Training printout
"""

hparams = {
    'learning_rate': 0.001,
    'epochs':        10,
    'train_size':    6036, # max 6036
    'test_size':     750,  # max 750
    'logint':        1,   # frequency of printing result every epoch
    'print_pred':    0,   # frequency of printing sentence in one epoch, 0 don't print
    'train_data':    prevsent_train,
    'test_data':     prevsent_dev,
    'plot':          True,
    'rand_seed':     666,
    'gpu':           True,
    'loss_type':     'cross_entropy'}
error_records_prev = evalprev()

"""### 4.1.2 Description

Compared to the test acc from EVALLM on singal sentence, 0.328, the result here, 0.356, is indeed higher!

## 4.2 top 35 errors
"""

error_records_prev.print_freq_error(35)

"""## 4.3 Categorize

I categorized as the following groups, similar to section 2.3

- Subjects such as (He, Bob)
    - 4, 5, 12, 13, 30, 32
- possibly ending point in a sentence and complementary such as (and, .)
    - 1, 2, 8, 11, 14, 17, 19, 20, 26, 27, 28, 29, 35
- verbs and tense such as (got, was)
    - 6, 16, 23, 25, 31
- 's, his, her, the, a
    - 7, 9, 10, 18, 21
    
It makes less sbuject errors now, comparing to the singal sentence model. Because it remembers the last sentence and knows the context better.

# 5 Hinge loss

## 5.1 Training printout

### 5.1.1 $r$
"""

hparams = {
    'learning_rate': 0.001,
    'epochs':        20,
    'train_size':    100, # max 6036
    'test_size':     10,  # max 750
    'logint':        1,   # frequency of printing result every epoch
    'print_pred':    0,   # frequency of printing sentence in one epoch, 0 don't print
    'train_data':    lm_train,
    'test_data':     lm_dev,
    'plot':          True,
    'rand_seed':     555,
    'gpu':           True,
    'loss_type':     'contrastive_hinge_loss',
    'r':             20,
    'f':             0,}
error_records = evallm()

hparams = {
    'learning_rate': 0.001,
    'epochs':        10,
    'train_size':    100, # max 6036
    'test_size':     10,  # max 750
    'logint':        1,   # frequency of printing result every epoch
    'print_pred':    0,   # frequency of printing sentence in one epoch, 0 don't print
    'train_data':    lm_train,
    'test_data':     lm_dev,
    'plot':          True,
    'rand_seed':     555,
    'gpu':           True,
    'loss_type':     'contrastive_hinge_loss',
    'r':             100,
    'f':             0,}
error_records = evallm()

hparams = {
    'learning_rate': 0.001,
    'epochs':        10,
    'train_size':    100, # max 6036
    'test_size':     10,  # max 750
    'logint':        1,   # frequency of printing result every epoch
    'print_pred':    0,   # frequency of printing sentence in one epoch, 0 don't print
    'train_data':    lm_train,
    'test_data':     lm_dev,
    'plot':          True,
    'rand_seed':     555,
    'gpu':           True,
    'loss_type':     'contrastive_hinge_loss',
    'r':             500,
    'f':             0,}
error_records = evallm()

"""### 5.1.2 $f$"""

hparams = {
    'learning_rate': 0.001,
    'epochs':        20,
    'train_size':    100, # max 6036
    'test_size':     10,  # max 750
    'logint':        1,   # frequency of printing result every epoch
    'print_pred':    0,   # frequency of printing sentence in one epoch, 0 don't print
    'train_data':    lm_train,
    'test_data':     lm_dev,
    'plot':          True,
    'rand_seed':     555,
    'gpu':           True,
    'loss_type':     'contrastive_hinge_loss',
    'r':             100,
    'f':             1/2,}
error_records = evallm()

hparams = {
    'learning_rate': 0.001,
    'epochs':        20,
    'train_size':    100, # max 6036
    'test_size':     10,  # max 750
    'logint':        1,   # frequency of printing result every epoch
    'print_pred':    0,   # frequency of printing sentence in one epoch, 0 don't print
    'train_data':    lm_train,
    'test_data':     lm_dev,
    'plot':          True,
    'rand_seed':     555,
    'gpu':           True,
    'loss_type':     'contrastive_hinge_loss',
    'r':             100,
    'f':             1,}
error_records = evallm()

hparams['f'] = 3/4
_ = evallm()

"""## 5.2 Description

with f = 0
- r = 20, acc = 0.105
- r = 100, acc = 0.097
- r = 500, acc = 0.097

with r = 100
- f = 1/2, acc = 0.097
- f = 1, acc = 0.065
- f = 3/4, acc = 0.016

We acn see that the accuracy is better for r = 20 and f = 0. However, this just used a small dataset (1/6) without small learning rate, more experiments would needed.

# Appendix

Please ignore these trivial training results that from my own experiments.
"""

hparams = {
    'learning_rate': 0.001,
    'epochs':        20,
    'train_size':    1000, # max 6036
    'test_size':     100,  # max 750
    'logint':        1,   # frequency of printing result every epoch
    'print_pred':    0,   # frequency of printing sentence in one epoch, 0 don't print
    'train_data':    lm_train,
    'test_data':     lm_dev,
    'plot':          True,
    'rand_seed':     555,
    'gpu':           True,
    'loss_type':     'contrastive_hinge_loss',
    'r':             20,
    'f':             0,}
error_records = evallm()

hparams = {
    'learning_rate': 0.001,
    'epochs':        10,
    'train_size':    6036, # max 6036
    'test_size':     750,  # max 750
    'logint':        1,   # frequency of printing result every epoch
    'print_pred':    0,   # frequency of printing sentence in one epoch, 0 don't print
    'train_data':    prevsent_train,
    'test_data':     prevsent_dev,
    'plot':          True,
    'rand_seed':     555,
    'gpu':           True,
    'loss_type':     'cross_entropy'}
error_records_prev = evalprev()

hparams = {
    'learning_rate': 0.0001,
    'epochs':        20,
    'train_size':    6036, # max 6036
    'test_size':     750,  # max 750
    'logint':        1,   # frequency of printing result every epoch
    'print_pred':    0,   # frequency of printing sentence in one epoch, 0 don't print
    'train_data':    lm_train,
    'test_data':     lm_dev,
    'plot':          True,
    'rand_seed':     555,
    'gpu':           True,
    'loss_type':     'contrastive_hinge_loss',
    'r':             5,
    'f':             0,}
error_records = evallm()

"""# References

* [Sequence Models and Long-Short Term Memory Networks — PyTorch Tutorials 1.0.0.dev20190327 documentation](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html)
* [穿越時空的偉人：用PyTorch重現偉人們的神經網絡
](https://pyliaorachel.github.io/blog/tech/nlp/2017/12/24/resurrecting-the-dead-chinese.html)
* [Understanding LSTM Networks -- colah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
* [How to use Pre-trained Word Embeddings in PyTorch – Martín Pellarolo – Medium](https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76)
* [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)
* [Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM) - YouTube](https://www.youtube.com/watch?v=WCUNPb-5EYI)
* [examples/train.py at master · pytorch/examples](https://github.com/pytorch/examples/blob/master/snli/train.py)
* [examples/main.py at master · pytorch/examples](https://github.com/pytorch/examples/blob/master/word_language_model/main.py)
* [Creating A Text Generator Using Recurrent Neural Network - Chun’s Machine Learning Page](https://chunml.github.io/ChunML.github.io/project/Creating-Text-Generator-Using-Recurrent-Neural-Network/)
* [LSTM Neural Network from Scratch | Kaggle](https://www.kaggle.com/navjindervirdee/lstm-neural-network-from-scratch)
* [lstm/lstm.py at master · nicodjimenez/lstm](https://github.com/nicodjimenez/lstm/blob/master/lstm.py)
* [LSTMs for Time Series in PyTorch | Jessica Yung](http://www.jessicayung.com/lstms-for-time-series-in-pytorch/)
* [tutorials/sequence_models_tutorial.py at master · pytorch/tutorials](https://github.com/pytorch/tutorials/blob/master/beginner_source/nlp/sequence_models_tutorial.py)
* [学习笔记CB012: LSTM 简单实现、完整实现、torch、小说训练word2vec lstm机器人 - OurCoders (我们程序员)](http://ourcoders.com/thread/show/9491/)
* [Long short-term memory - Wikipedia](https://en.wikipedia.org/wiki/Long_short-term_memory)
"""